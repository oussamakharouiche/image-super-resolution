W0518 17:15:46.190000 5709 torch/distributed/run.py:792] 
W0518 17:15:46.190000 5709 torch/distributed/run.py:792] *****************************************
W0518 17:15:46.190000 5709 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 17:15:46.190000 5709 torch/distributed/run.py:792] *****************************************
[Rank 0] Using device: cuda:0
[Rank 2] Using device: cuda:2
[Rank 1] Using device: cuda:1
[Rank 3] Using device: cuda:3
3f46c0a999b5:5766:5766 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
3f46c0a999b5:5766:5766 [0] NCCL INFO Bootstrap : Using eth0:172.17.0.3<0>
3f46c0a999b5:5766:5766 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
3f46c0a999b5:5766:5766 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
3f46c0a999b5:5766:5766 [0] NCCL INFO NET/Plugin: Using internal network plugin.
3f46c0a999b5:5766:5766 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.21.5+cuda12.4
3f46c0a999b5:5766:5766 [0] NCCL INFO Comm config Blocking set to 1
3f46c0a999b5:5768:5768 [2] NCCL INFO cudaDriverVersion 12040
3f46c0a999b5:5768:5768 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
3f46c0a999b5:5768:5768 [2] NCCL INFO Bootstrap : Using eth0:172.17.0.3<0>
3f46c0a999b5:5768:5768 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
3f46c0a999b5:5768:5768 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
3f46c0a999b5:5768:5768 [2] NCCL INFO NET/Plugin: Using internal network plugin.
3f46c0a999b5:5768:5768 [2] NCCL INFO Comm config Blocking set to 1
3f46c0a999b5:5767:5767 [1] NCCL INFO cudaDriverVersion 12040
3f46c0a999b5:5767:5767 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
3f46c0a999b5:5767:5767 [1] NCCL INFO Bootstrap : Using eth0:172.17.0.3<0>
3f46c0a999b5:5767:5767 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
3f46c0a999b5:5767:5767 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
3f46c0a999b5:5767:5767 [1] NCCL INFO NET/Plugin: Using internal network plugin.
3f46c0a999b5:5767:5767 [1] NCCL INFO Comm config Blocking set to 1
3f46c0a999b5:5769:5769 [3] NCCL INFO cudaDriverVersion 12040
3f46c0a999b5:5769:5769 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
3f46c0a999b5:5769:5769 [3] NCCL INFO Bootstrap : Using eth0:172.17.0.3<0>
3f46c0a999b5:5769:5769 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
3f46c0a999b5:5769:5769 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
3f46c0a999b5:5769:5769 [3] NCCL INFO NET/Plugin: Using internal network plugin.
3f46c0a999b5:5769:5769 [3] NCCL INFO Comm config Blocking set to 1
3f46c0a999b5:5766:5791 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
3f46c0a999b5:5766:5791 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
3f46c0a999b5:5766:5791 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3<0>
3f46c0a999b5:5766:5791 [0] NCCL INFO Using non-device net plugin version 0
3f46c0a999b5:5766:5791 [0] NCCL INFO Using network Socket
3f46c0a999b5:5768:5792 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
3f46c0a999b5:5768:5792 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
3f46c0a999b5:5768:5792 [2] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3<0>
3f46c0a999b5:5768:5792 [2] NCCL INFO Using non-device net plugin version 0
3f46c0a999b5:5768:5792 [2] NCCL INFO Using network Socket
3f46c0a999b5:5767:5793 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
3f46c0a999b5:5767:5793 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
3f46c0a999b5:5767:5793 [1] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3<0>
3f46c0a999b5:5767:5793 [1] NCCL INFO Using non-device net plugin version 0
3f46c0a999b5:5767:5793 [1] NCCL INFO Using network Socket
3f46c0a999b5:5769:5794 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
3f46c0a999b5:5769:5794 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
3f46c0a999b5:5769:5794 [3] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3<0>
3f46c0a999b5:5769:5794 [3] NCCL INFO Using non-device net plugin version 0
3f46c0a999b5:5769:5794 [3] NCCL INFO Using network Socket
3f46c0a999b5:5769:5794 [3] NCCL INFO ncclCommInitRank comm 0x55c4c7aaedc0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 5000 commId 0x1789eddb9d18832f - Init START
3f46c0a999b5:5768:5792 [2] NCCL INFO ncclCommInitRank comm 0x5586f517c880 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 4000 commId 0x1789eddb9d18832f - Init START
3f46c0a999b5:5766:5791 [0] NCCL INFO ncclCommInitRank comm 0x55dfdccd3dc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x1789eddb9d18832f - Init START
3f46c0a999b5:5767:5793 [1] NCCL INFO ncclCommInitRank comm 0x55c61d321a80 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 3000 commId 0x1789eddb9d18832f - Init START
3f46c0a999b5:5769:5794 [3] NCCL INFO Setting affinity for GPU 3 to 03ff,f0003fff
3f46c0a999b5:5767:5793 [1] NCCL INFO Setting affinity for GPU 1 to 03ff,f0003fff
3f46c0a999b5:5767:5793 [1] NCCL INFO NVLS multicast support is not available on dev 1
3f46c0a999b5:5768:5792 [2] NCCL INFO Setting affinity for GPU 2 to 03ff,f0003fff
3f46c0a999b5:5769:5794 [3] NCCL INFO NVLS multicast support is not available on dev 3
3f46c0a999b5:5766:5791 [0] NCCL INFO Setting affinity for GPU 0 to 03ff,f0003fff
3f46c0a999b5:5768:5792 [2] NCCL INFO NVLS multicast support is not available on dev 2
3f46c0a999b5:5766:5791 [0] NCCL INFO NVLS multicast support is not available on dev 0
3f46c0a999b5:5767:5793 [1] NCCL INFO comm 0x55c61d321a80 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
3f46c0a999b5:5766:5791 [0] NCCL INFO comm 0x55dfdccd3dc0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
3f46c0a999b5:5768:5792 [2] NCCL INFO comm 0x5586f517c880 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
3f46c0a999b5:5769:5794 [3] NCCL INFO comm 0x55c4c7aaedc0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
3f46c0a999b5:5767:5793 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
3f46c0a999b5:5767:5793 [1] NCCL INFO P2P Chunksize set to 131072
3f46c0a999b5:5766:5791 [0] NCCL INFO Channel 00/02 :    0   1   2   3
3f46c0a999b5:5766:5791 [0] NCCL INFO Channel 01/02 :    0   1   2   3
3f46c0a999b5:5768:5792 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
3f46c0a999b5:5769:5794 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
3f46c0a999b5:5766:5791 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
3f46c0a999b5:5768:5792 [2] NCCL INFO P2P Chunksize set to 131072
3f46c0a999b5:5766:5791 [0] NCCL INFO P2P Chunksize set to 131072
3f46c0a999b5:5769:5794 [3] NCCL INFO P2P Chunksize set to 131072
3f46c0a999b5:5767:5793 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
3f46c0a999b5:5767:5793 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
3f46c0a999b5:5766:5791 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
3f46c0a999b5:5766:5791 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
3f46c0a999b5:5768:5792 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
3f46c0a999b5:5769:5794 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
3f46c0a999b5:5769:5794 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
3f46c0a999b5:5768:5792 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct
3f46c0a999b5:5766:5791 [0] NCCL INFO Connected all rings
3f46c0a999b5:5767:5793 [1] NCCL INFO Connected all rings
3f46c0a999b5:5769:5794 [3] NCCL INFO Connected all rings
3f46c0a999b5:5768:5792 [2] NCCL INFO Connected all rings
3f46c0a999b5:5769:5794 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct
3f46c0a999b5:5769:5794 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct
3f46c0a999b5:5767:5793 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
3f46c0a999b5:5767:5793 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
3f46c0a999b5:5768:5792 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct
3f46c0a999b5:5768:5792 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct
3f46c0a999b5:5766:5791 [0] NCCL INFO Connected all trees
3f46c0a999b5:5766:5791 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
3f46c0a999b5:5766:5791 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
3f46c0a999b5:5767:5793 [1] NCCL INFO Connected all trees
3f46c0a999b5:5767:5793 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
3f46c0a999b5:5767:5793 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
3f46c0a999b5:5769:5794 [3] NCCL INFO Connected all trees
3f46c0a999b5:5769:5794 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
3f46c0a999b5:5769:5794 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
3f46c0a999b5:5768:5792 [2] NCCL INFO Connected all trees
3f46c0a999b5:5768:5792 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
3f46c0a999b5:5768:5792 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
3f46c0a999b5:5769:5794 [3] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
3f46c0a999b5:5767:5793 [1] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
3f46c0a999b5:5769:5794 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
3f46c0a999b5:5767:5793 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
3f46c0a999b5:5769:5794 [3] NCCL INFO ncclCommInitRank comm 0x55c4c7aaedc0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 5000 commId 0x1789eddb9d18832f - Init COMPLETE
3f46c0a999b5:5767:5793 [1] NCCL INFO ncclCommInitRank comm 0x55c61d321a80 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 3000 commId 0x1789eddb9d18832f - Init COMPLETE
3f46c0a999b5:5766:5791 [0] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
3f46c0a999b5:5766:5791 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
3f46c0a999b5:5766:5791 [0] NCCL INFO ncclCommInitRank comm 0x55dfdccd3dc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 2000 commId 0x1789eddb9d18832f - Init COMPLETE
3f46c0a999b5:5768:5792 [2] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
3f46c0a999b5:5768:5792 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
3f46c0a999b5:5768:5792 [2] NCCL INFO ncclCommInitRank comm 0x5586f517c880 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 4000 commId 0x1789eddb9d18832f - Init COMPLETE
/venv/main/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/venv/main/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/venv/main/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/venv/main/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/venv/main/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/venv/main/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/venv/main/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/venv/main/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/image-super-resolution/train.py", line 106, in <module>
[rank0]:     train(rank=int(os.environ['LOCAL_RANK']), world_size=torch.cuda.device_count())
[rank0]:   File "/workspace/image-super-resolution/train.py", line 78, in train
[rank0]:     loss = model(x_hr, x_lr)
[rank0]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1639, in forward
[rank0]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank0]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1528, in _pre_forward
[rank0]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank0]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank0]: making sure all `forward` function outputs participate in calculating loss. 
[rank0]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank0]: Parameter indices which did not receive grad for rank 0: 362 363 364 365 366 389 390 391 392 393 416 417 418 419 420 443 444 445 446 447 470 471 472 473 474 497 498 499 500 501
[rank0]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank3]: Traceback (most recent call last):
[rank3]:   File "/workspace/image-super-resolution/train.py", line 106, in <module>
[rank3]:     train(rank=int(os.environ['LOCAL_RANK']), world_size=torch.cuda.device_count())
[rank3]:   File "/workspace/image-super-resolution/train.py", line 78, in train
[rank3]:     loss = model(x_hr, x_lr)
[rank3]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1639, in forward
[rank3]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank3]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1528, in _pre_forward
[rank3]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank3]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank3]: making sure all `forward` function outputs participate in calculating loss. 
[rank3]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank3]: Parameter indices which did not receive grad for rank 3: 362 363 364 365 366 389 390 391 392 393 416 417 418 419 420 443 444 445 446 447 470 471 472 473 474 497 498 499 500 501
[rank3]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank2]: Traceback (most recent call last):
[rank2]:   File "/workspace/image-super-resolution/train.py", line 106, in <module>
[rank2]:     train(rank=int(os.environ['LOCAL_RANK']), world_size=torch.cuda.device_count())
[rank2]:   File "/workspace/image-super-resolution/train.py", line 78, in train
[rank2]:     loss = model(x_hr, x_lr)
[rank2]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1639, in forward
[rank2]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank2]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1528, in _pre_forward
[rank2]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank2]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank2]: making sure all `forward` function outputs participate in calculating loss. 
[rank2]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank2]: Parameter indices which did not receive grad for rank 2: 362 363 364 365 366 389 390 391 392 393 416 417 418 419 420 443 444 445 446 447 470 471 472 473 474 497 498 499 500 501
[rank2]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank1]: Traceback (most recent call last):
[rank1]:   File "/workspace/image-super-resolution/train.py", line 106, in <module>
[rank1]:     train(rank=int(os.environ['LOCAL_RANK']), world_size=torch.cuda.device_count())
[rank1]:   File "/workspace/image-super-resolution/train.py", line 78, in train
[rank1]:     loss = model(x_hr, x_lr)
[rank1]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1639, in forward
[rank1]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank1]:   File "/venv/main/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1528, in _pre_forward
[rank1]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank1]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank1]: making sure all `forward` function outputs participate in calculating loss. 
[rank1]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank1]: Parameter indices which did not receive grad for rank 1: 362 363 364 365 366 389 390 391 392 393 416 417 418 419 420 443 444 445 446 447 470 471 472 473 474 497 498 499 500 501
[rank1]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank0]:[W518 17:15:55.570870626 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
3f46c0a999b5:5766:5796 [0] NCCL INFO [Service thread] Connection closed by localRank 0
3f46c0a999b5:5769:5798 [3] NCCL INFO [Service thread] Connection closed by localRank 3
3f46c0a999b5:5768:5797 [2] NCCL INFO [Service thread] Connection closed by localRank 2
3f46c0a999b5:5767:5795 [1] NCCL INFO [Service thread] Connection closed by localRank 1
3f46c0a999b5:5769:5888 [3] NCCL INFO comm 0x55c4c7aaedc0 rank 3 nranks 4 cudaDev 3 busId 5000 - Abort COMPLETE
3f46c0a999b5:5766:5887 [0] NCCL INFO comm 0x55dfdccd3dc0 rank 0 nranks 4 cudaDev 0 busId 2000 - Abort COMPLETE
3f46c0a999b5:5768:5889 [2] NCCL INFO comm 0x5586f517c880 rank 2 nranks 4 cudaDev 2 busId 4000 - Abort COMPLETE
3f46c0a999b5:5767:5890 [1] NCCL INFO comm 0x55c61d321a80 rank 1 nranks 4 cudaDev 1 busId 3000 - Abort COMPLETE
W0518 17:15:57.063000 5709 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 5767 closing signal SIGTERM
W0518 17:15:57.064000 5709 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 5768 closing signal SIGTERM
W0518 17:15:57.065000 5709 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 5769 closing signal SIGTERM
E0518 17:15:57.280000 5709 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 5766) of binary: /venv/main/bin/python3.10
Traceback (most recent call last):
  File "/venv/main/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/venv/main/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/venv/main/lib/python3.10/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/venv/main/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/venv/main/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/venv/main/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-18_17:15:57
  host      : 3f46c0a999b5
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 5766)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
